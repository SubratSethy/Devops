Introduction to docker.
->Virtual Machine is advancement to Physical Servers 
->Containers is advancement to virtual machines.
Concept of virtual machine :
->When we using our personal personal laptop or physical servers we are not completely using the available resources -> CPU,RAM and hardwares etc.Organizations have thousands of servers.They are paying for these resources.
->To solve this -> Hypervisor used -> Hypervisor uses the concept of Virtualization -> Virtualiazation used to create lot of virtual machines (vm)or virtual servers on top of your physical servers.
                                                                                    -> vm is a virtual isolation of ps. -> each vms has its own operating system(os).
                                                                                                                        -> Because of separate os vms are logically separated from each other-> results in vms are very secure.  
                                                                                                                        -> on top of this os we run our application.
->Instead of running a application on a physical servers we can run a lot applications on lot of vms on a single physical servers.
->AWS install zen hypervisor on its physical servers->create EC2 instances(virtual machines).->while creating vms you define os(what is ram and cpu of vms)

Then why should we move to containers ?
-> while running applications on full capacity on vms it not uses 25 CPU,25GB RAM, we are not utilizing the complete resources of vms (ps:100CPU,100GB RAM ->into 4 vms each of 25CPU,25GB RAM).
-> So containers came to solve this problem.Problems of ps solved to some extent by vms ->Problems of vms solved to some extent by containers.
-> Containers doesnot have complete Operating System so they are not complete isolate from each other so there is security issue.
Architecture of Containers.
->Containers can be created by installing containorization platform example: Docker 
->2 ways of creating containers.->model 1:installing containorization platform example: Docker on physical servers which has os already installed on it .
                                ->model 2:installing containorization platform example: Docker on vms which are installed on physical servers which has os already installed on it.
                                ->Now-a days everyone using model-2 as it is based on vms and cloud as there is less maintaince as we are using vms.
                                ->for model-1 we have do maintain a system administrator for security fixes and new pathes we have to maintain a datacenter. 
->Containers are  light weight in nature as they donnot have complete os.->They use the required resources form base os or host os of vms and physical servers they are installed on.
->Containers have minimal os and base image-> containers are package of appliaction,application libraries, sytsem dependencies->your applicatioon may require system dependencies that may be python, java etc.
->If we require any other shared libraries except that present on containers that will be used from host os. 
->If we want to create image of your vm we will create a snapshot(image) of our vm ->Size of snapshots(image) may be 1GB(minimal with all dependencies are installed)or may be 2GB.
->If we consider vm with container then size of the snapshots of container fall between 100(minimal) to 500 MB(it increases to 500 if application has lot of dependencies )
->so there is significant drop in the size when we consider container.As conbtainer lightweight->easy to ship and deploy one docker container from one containerzation platform to another platform and deploy image to kubernetes.
->In docker image we use a base image that contains system dependencies,application and application libraries ->they will form docker image or docker container.->so,containers have minimalstic os.
Docker LifeCycle
->first write docker file ->using docker commands sent this file to docker engine->use docker commands to create docker image from docker file->write commands to create docker container from docker image.
                                                                                  -> docker build command                                     -> docker run command
->Docker is very much dependent on docker engine if docker fails then all the conatainers will go down it is single point of failure.
->When docker image ios created it creates a lot of layers so it take lot memory space.
->To solve these problems Buildah comes -> we write shellscript and put all buildah commands in it->it will create docker image.
How to push the docker image into a public and private registry by doing this we share this to customers.
How to reduce the size of the docker image by definition  the size of the image should be small but we by keep adding packages to docker image its size get increased.
->when containers not running they donot use resources from the kernal(part of host opertaing system)
->when vms or EC2 instances are created they have os with kernal.
->All the containers have some system dependencies it should be there as there should be some part of logoical isolation between the containers as they can use all resources from kernal any hacker can get into kubernetes cluster
 and in one container can get into all the containers.
->To provide a better picture of files and folders that containers base images have and files and folders that containers use from host operating system (not 100 percent accurate -> varies from base image to base image). 
  Refer below.

Files and Folders in containers base images
->These files and folders make a logical isolation from one container to another-> one container doesnot share this with another container.These files and folders doesnot use by the kernal these are part container. 
    /bin: contains binary executable files, such as the ls, cp, and ps commands.

    /sbin: contains system binary executable files, such as the init and shutdown commands.

    /etc: contains configuration files for various system services.

    /lib: contains library files that are used by the binary executables.

    /usr: contains user-related files and utilities, such as applications, libraries, and documentation.

    /var: contains variable data, such as log files, spool files, and temporary files.

    /root: is the home directory of the root user.
Files and Folders that containers use from host operating system
->These are the part of the kernal or host operting system.
    The host's file system: Docker containers can access the host file system using bind mounts, which allow the container to read and write files in the host file system.

    Networking stack: The host's networking stack is used to provide network connectivity to the container. Docker containers can be connected to the host's network directly or through a virtual network.

    System calls: The host's kernel handles system calls from the container, which is how the container accesses the host's resources, such as CPU, memory, and I/O.

    Namespaces: Docker containers use Linux namespaces to create isolated environments for the container's processes. Namespaces provide isolation for resources such as the file system, process ID, and network.

    Control groups (cgroups): Docker containers use cgroups to limit and control the amount of resources, such as CPU, memory, and I/O, that a container can access.
->you can run n number of containers on your vms depending on how much resources they are using from host os->when onbe container is not running they can allow other containers to use the resources from host os.
->As vms have individual os vm1 can not use kernal related resources from vm2.
    
What is Docker ?
Docker is a containerization platform that provides easy way to containerize your applications, which means, using Docker you can build container images, run the images to create containers and also push these containers to container regestries such as DockerHub, Quay.io and so on.

In simple words, you can understand as containerization is a concept or technology and Docker Implements Containerization.

Docker Architecture ?
1.Docker Client
when we use Docker Client or Docker CLI we execute some commands that received by docker daemon(when installing docker we actually installing docker daemon).->dockerdaemon executes all the commands gives output as docker image and docker container.
->By using Docker daemon we push docker container into docker registry by using docker commands.
->If docker daemon goes down then all containers and docker will not work or goes down.
2.Docker Host contain docker daemon.
3.Docker Regisrty
Docker LifeCycle
There are three important things,
1.docker build -> builds docker images from Dockerfile->Docker daemon executes this command given as input by docker cli.
2.docker run -> runs container from docker images 
3.docker push -> push the container image to public/private regestries to share the docker images or docker container.
Daemon service LifeCycle:
Daemon receives request from docker cli.
->first as a user you have to  write a docker file->this file contains set of instructions for example you tell daemon to create ubuntu base image and put the given source code into base image
->run specific command->if it is node js run npm command if it is python run pip command-> so that all application libraries and system dependencies get installed inside the image->run set of commands so that application executes.
->use docker build to submit the docker file to docker daemon to create docker daemon->docker run to create docker container as output it contains application libraries,system dependencies.
->By public registry you send containers to uers they download it use it.
->If we do manually first download the application and create vms and run on it->export the port and lot of things
->these problems solved by the docker->helps to reduce complexity->increase efficiency as execute docker image all the things get created as docker image contains all the things.
Understanding the terminology (Inspired from Docker Docs)
Docker daemon
The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.

Docker client
The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. 
The docker command uses the Docker API. The Docker client can communicate with more than one daemon.

Docker Desktop
Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices.
Docker Desktop includes the Docker daemon (dockerd), the Docker client (docker), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see Docker Desktop.

Docker registries
A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry.
you create private registry within Docker hub which can be accessed by some users with valid permissions.

When you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry. Docker objects

When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.

Dockerfile
Dockerfile is a file where you provide the steps to build your Docker Image.

Images
An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.

You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.
->If you want to run a python application on ps->download python enviornment and install other requirements->run application
->If you want to run on vm ->create a vm->create vm image->install python and other requirements->then run application
->If you want to use docker->After deployment user download docker image -> use docker command docker run to run apllication.
You can create an Ubuntu EC2 Instance on AWS and run the below commands to install docker.

sudo apt update->used update the repo.
sudo is used to get access to the reposirtry.
sudo apt install docker.io -y->install docker on vm->-y stands for user who is installing is agree with it.
apt stands for Advanced Package Tool. It is a command-line package management utility used in Debian-based Linux distributions, such as Ubuntu. 
->start Docker daemon:
You use the below command to verify if the docker daemon is actually started and Active
sudo systemctl status docker

If you notice that the docker daemon is not running, you can start the daemon using the below command

sudo systemctl start docker
->docker daemon runs with root user 
->docker have to be installs using root user bydefault .
->so have to give permission to root user
->docker is runnig as root->so no access to ubutu user.
Grant Access to your user to run docker commands
To grant access to your user to run the docker command, you should add the user to the Docker Linux group. Docker group is create by default when docker is installed.

sudo usermod -aG docker ubuntu
->if you are using personal laptop instead of vm you can use docker hub which basiaclly creates vm for you and install docker on it.
->first we will use ubuntu image:latest.
->we have to set the working directory in the image->COPY./app 
->app is working folder.
->copy the file system from host file stystem to image file system ->COPY ./app->we will transfer all the command files from the host file image file or working directory as all the commands will get executed inside the image.
->docker build command will execute all the commands that you provide in docker file in a sequential way 


Clone this repository and move to example folder
git clone https://github.com/iam-veeramalla/Docker-Zero-to-Hero
cd  examples
Login to Docker [Create an account with https://hub.docker.com/]
docker login
Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: abhishekf5
Password:
WARNING! Your password will be stored unencrypted in /home/ubuntu/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded
Build your first Docker Image
You need to change the username accoringly in the below command

docker build -t abhishekf5/my-first-docker-image:latest .
Output of the above command

    Sending build context to Docker daemon  992.8kB
    Step 1/6 : FROM ubuntu:latest
    latest: Pulling from library/ubuntu
    677076032cca: Pull complete
    Digest: sha256:9a0bdde4188b896a372804be2384015e90e3f84906b750c1a53539b585fbbe7f
    Status: Downloaded newer image for ubuntu:latest
     ---> 58db3edaf2be
    Step 2/6 : WORKDIR /app
     ---> Running in 630f5e4db7d3
    Removing intermediate container 630f5e4db7d3
     ---> 6b1d9f654263
    Step 3/6 : COPY . /app
     ---> 984edffabc23
    Step 4/6 : RUN apt-get update && apt-get install -y python3 python3-pip
     ---> Running in a558acdc9b03
    Step 5/6 : ENV NAME World
     ---> Running in 733207001f2e
    Removing intermediate container 733207001f2e
     ---> 94128cf6be21
    Step 6/6 : CMD ["python3", "app.py"]
     ---> Running in 5d60ad3a59ff
    Removing intermediate container 5d60ad3a59ff
     ---> 960d37536dcd
    Successfully built 960d37536dcd
    Successfully tagged abhishekf5/my-first-docker-image:latest
Verify Docker Image is created
docker images
Output

REPOSITORY                         TAG       IMAGE ID       CREATED          SIZE
abhishekf5/my-first-docker-image   latest    960d37536dcd   26 seconds ago   467MB
ubuntu                             latest    58db3edaf2be   13 days ago      77.8MB
hello-world                        latest    feb5d9fea6a5   16 months ago    13.3kB
Run your First Docker Container
docker run -it abhishekf5/my-first-docker-image
Output

Hello World
Push the Image to DockerHub and share it with the world
docker push abhishekf5/my-first-docker-image
Output

Using default tag: latest
The push refers to repository [docker.io/abhishekf5/my-first-docker-image]
896818320e80: Pushed
b8088c305a52: Pushed
69dd4ccec1a0: Pushed
c5ff2d88f679: Mounted from library/ubuntu
latest: digest: sha256:6e49841ad9e720a7baedcd41f9b666fcd7b583151d0763fe78101bb8221b1d88 size: 1157
->
->
Install django 
first install python->install django using pip command.->import django command to verify whether it is installed or not.
then install django admin and print(django.get_version()) if it is installed it will give version of django.
django admin very much similar to ansible galaxy ->ansible galaxy creates a skeleton of the ansible role->similarly dajango admin used to create skeleton when we create project
and write microservices using django admin.
-> command is django admin startproject devops->it creates devops(project name) named directory in the current directory and creates skeleton.
->inside the devops directory there are some files that are project configuration.example:settings.py is file that contains what are the ipos that are to connect 
and what arae the database you want to connect,secrest keys and any secure information and any django middlewares taht you want to use and any supported templetes.
it will set the entire configuration for the django project.
->uris.py file uses for serving the content.
->instance ip address/port number/demo/->/demo is the context root of our application.Any body who types /demo Url.py will understand that it have to show the content of demo application.
till now we created a base to bulid our application not yet built the application
->to create application command is python manage.py startapp demo (application name).It creates a bunch of files in demo directory.
->In view.py we will write our code.->IN this file a python function is written index function, it is rendering a simple html file which is stored in a folder called templete 
and from this the content get served to customer when customer did request.This is the overall workflow of django application.
->When you do all the changes and prepared the complete application you have to send to QE team Before the exitstance of docker->QE team have to download the github reposiritory 
or the artifact that you created and crreate dependencies(number of dependencies may vary depending on the application) that present in the requirements.txt file and install them.
then the actual problem starts the QE team may be on the Linux,windowsor may be on Linux sytem may be using the different distributions of Liunx for that case our commands may not work you will say
the everything is correct but QE team will say the application not running.This is solved by the Docker.
->In container we are bundling and packaging everying in the container so that application will run only the Qe team have to install Docker on their machine and use the commands
docker bulid and docker run -> applcationn will run fine -> because it has the system dependencies in itself that are required to run your application for example your application requires 
python and pip command to run that will be present within the container along with the source code that is the binary of your application.only in some cases it requires some resources from kernal and 
in case of system calls also it uses resources from kernal.
-> for a devops enngineer first thing to conatinarize a application you have to first write a docker file
-> for user thy have to clone the dockerfile to the reposiritory and run commands dockerbuild and docker run and application will run on their machine.
-> for a devops engineer first thing you have to do select a base image here i choose ubutu base image.you can use centois base image. 

->RUN apt-get update && \
    apt-get install -y python3 python3-pip && \  ->this is for ubutu ->if you use centois inplace of apt use  am.
->from ubutu i choose work directory(where source code get saved).->then move the requirements.txt file(all the dependencies that require to run our application) into the working directory ./app.
WORKDIR /app

COPY requirements.txt /app
COPY devops /app
->this command will copy the source code the .app/ directory.->after combination of source code and all dependencies will form a bundle or package or binary of our application.
->as we are using ubuntu so we have install python to run python application->if we use FROM Python no need to install python.
RUN apt-get update && \
    apt-get install -y python3 python3-pip && \

->django and tzdata are dependencies that are present insside the requirements.txt file these are required to run our python application.
 pip install -r requirements.txt && \
    cd devops
these commands are used to install all the dependencies.now our configuration is done now our application ready to use.





FROM ubuntu

WORKDIR /app

COPY requirements.txt /app
COPY devops /app

RUN apt-get update && \
    apt-get install -y python3 python3-pip && \
c

ENTRYPOINT ["python3"]
CMD ["manage.py", "runserver", "0.0.0.0:8000"]
all the contents inside ENTRYPOINT are not rewriteable i.e; doesnot change.
here python3 is there because to run pyhton appliaction it is needed.
CMD is configurable i.e; we can change the contents inside it.
if someone used the port number 8000 to run any other application the port 8000 can not be used to run this python application then user have to change the port number to run the python application.
because the container given by the server is running on port number 8000.

example/python-web-app docker build . this command will build our appliaction.
docker images  command will show the docker image or application that already built.
docker run will run our docker container. 
after this also our appliaction will not run beacause our appliaction is running in container and my container is at 8000 port but we are trying to access the django appliaction on 
EC2 instance not on container. so we need to do port mapping from container to host by this command: docker run -p 8000:8000 
after this our django application will run on our machine.



bind  mounts : allows you bind a  specific directory on the host with the specific folder on the container.if a new container came you cxan find the folder of it with the existing .app/ folder of host.
->binds folder on the container with one folder on the host let's say ./app->containbesr canm read that folder on the host.same is in the case of writing container writes inside its on folder
if the container goes down we can found the all the formation on the host.
->container wants to read folder information that is on host but without bind mounbts is not possible container only can use resources like cpu,memory,storage etc from host operating system.
volumes : also does same kind of solution but provides a better lifecycle.
docker cli-> docker command-> create volume on host ->volume is partition that you creating on host->you can create volume and destroy a volume->you can take volume from one container and ten
attach it to the another container.->at same point of time you can attach a volume to both of the containers.
by using volume you are attaching a specific folder or file system to the container and advantage is not provide the directory details that is not saying ./app folder on host should assign
to the folder on container instead you telling docker to create volume on host. df command create disk and volume will create volumesonn host will get mounted to the container.









 

